{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062ac96d",
   "metadata": {},
   "source": [
    "# Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15b0c3e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm  #progress bars for loops and iterables\n",
    "import math\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a4afaf",
   "metadata": {},
   "source": [
    "# MaxCut Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00820d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read graph file, e.g., BarabasiAlbert_100_ID2, using networkx.Graph\n",
    "def read_nxgraph(filename: str) -> nx.Graph():\n",
    "    graph = nx.Graph()\n",
    "    with open(filename, 'r') as file:\n",
    "        # lines = []\n",
    "        line = file.readline()\n",
    "        is_first_line = True\n",
    "        while line is not None and line != '':\n",
    "            if '/' not in line:\n",
    "                if is_first_line:\n",
    "                    strings = line.split(\" \")\n",
    "                    num_nodes = int(strings[0])\n",
    "                    num_edges = int(strings[1])\n",
    "                    nodes = list(range(num_nodes))\n",
    "                    graph.add_nodes_from(nodes)\n",
    "                    is_first_line = False\n",
    "                else:\n",
    "                    node1, node2, weight = line.split(\" \")\n",
    "                    graph.add_edge(int(node1) - 1, int(node2) - 1, weight=weight)\n",
    "            line = file.readline()\n",
    "    return graph\n",
    "\n",
    "# helper function that takes two batches of solutions and keep the better soltuion\n",
    "def update_best(cur_solutions, cur_scores, new_solutions, new_scores):\n",
    "    # get indices where new solution is better than current solution\n",
    "    better_indexes = new_scores.gt(cur_scores)\n",
    "    # update cur_solutions and cur_scores with the better solutions and scores\n",
    "    cur_solutions[better_indexes] = new_solutions[better_indexes]\n",
    "    cur_scores[better_indexes] = new_scores[better_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414947e",
   "metadata": {},
   "source": [
    "# MaxCut Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9796c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles cut-value calculation, local search, and initial solution sampling\n",
    "class MaxcutSimulator:\n",
    "    def __init__(self, file_name, device=torch.device(\"cpu\")):\n",
    "        self.graph = read_nxgraph(file_name)\n",
    "        self.num_nodes = self.graph.number_of_nodes()\n",
    "        self.device = device\n",
    "        # store the adjacency matrix\n",
    "        self.adj_matrix = torch.tensor(nx.to_numpy_array(self.graph), dtype=torch.float32, device=device)\n",
    "\n",
    "    # Calculate cut values for a batch of solutions\n",
    "    def get_cut_values(self, solutions):\n",
    "        # solutions with shape (batch_size, num_nodes)\n",
    "        batch_size, num_nodes = solutions.shape\n",
    "        s = solutions.reshape(batch_size, num_nodes, 1)\n",
    "        st = solutions.reshape(batch_size, 1, num_nodes)\n",
    "        res = (s != st).triu() * self.adj_matrix\n",
    "        scores = res.triu().sum((1, 2))\n",
    "        return scores # scores has shape (batch_size)\n",
    "\n",
    "    # Return the best solutions and scores found in applying greedy local search on a batch of solutions\n",
    "    def batched_local_search(self, solutions, scores, max_steps, num_flips, num_neighbors):\n",
    "        # solution has shape (batch_size, num_nodes)\n",
    "        # # scores has shape (batch_size)\n",
    "        '''\n",
    "        At each step, look at num_neighbors solutions, each num_flips bits different from the current solution,\n",
    "        and move to the best of these neighbors. Repeat max_steps times.\n",
    "        '''\n",
    "        batch_size = solutions.shape[0]\n",
    "        best_solutions = solutions.clone().to(self.device)\n",
    "        best_scores = scores.clone().to(self.device)\n",
    "        for step in range(max_steps):\n",
    "            new_solutions = best_solutions.clone().to(self.device)\n",
    "            # create num_neighbors copies of the current solutions\n",
    "            neighbors_solutions = new_solutions.unsqueeze(1)\\\n",
    "                                .repeat(1, num_neighbors, 1)\\\n",
    "                                .reshape(batch_size * num_neighbors, self.num_nodes)\n",
    "\n",
    "            # random flip indices (batch_size * num_neighbors, num_flips)\n",
    "            flip_indexes = torch.randint(0, self.num_nodes,\n",
    "                                        (batch_size * num_neighbors, num_flips),\n",
    "                                        device=self.device)\n",
    "            row_idx = torch.arange(batch_size * num_neighbors, device=self.device).unsqueeze(1).expand(-1, num_flips)\n",
    "\n",
    "            # flip the bits on the copies\n",
    "            neighbors_solutions[row_idx, flip_indexes] = 1 -\\\n",
    "                                                        neighbors_solutions[row_idx,\n",
    "                                                        flip_indexes]\n",
    "            new_scores = self.get_cut_values(neighbors_solutions)\\\n",
    "                .view(batch_size, num_neighbors)\n",
    "            # find best neighbors\n",
    "            max_scores, max_indices = new_scores.max(dim=1)\n",
    "            neighbors_solutions = neighbors_solutions.view(batch_size, num_neighbors, self.num_nodes)[torch.arange(batch_size), max_indices, :]\n",
    "\n",
    "            # compare best neighbors with current solutions. Move the the neighbor is it's better\n",
    "            update_best(best_solutions, best_scores, neighbors_solutions, max_scores)\n",
    "\n",
    "        return best_solutions, best_scores\n",
    "\n",
    "    def initialize_trajectories(self, trajectory_length, trajectories_per_epoch):\n",
    "        # initialize empty tensors to store batch of trajectories for an epoch\n",
    "        trajectory_solutions = torch.empty((trajectory_length, trajectories_per_epoch, self.num_nodes), dtype=torch.float32).to(self.device)\n",
    "        trajectory_scores = torch.empty((trajectory_length, trajectories_per_epoch), dtype=torch.float32).to(self.device)\n",
    "        trajectory_log_probs = torch.empty((trajectory_length,trajectories_per_epoch), dtype=torch.float32)\n",
    "        trajectory_advantages = torch.empty((trajectory_length, trajectories_per_epoch), dtype=torch.float32).to(self.device)\n",
    "        ls_trajectory_solutions = torch.empty((trajectory_length, trajectories_per_epoch, self.num_nodes), dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Random initialization of the start of new trajectories\n",
    "        trajectory_solutions[0, :, :] = torch.randint(2, (trajectories_per_epoch, self.num_nodes), dtype=torch.float32).to(self.device)\n",
    "        trajectory_scores[0, :] = self.get_cut_values(trajectory_solutions[0])\n",
    "        trajectory_log_probs[0, :] = torch.full((trajectories_per_epoch,), -self.num_nodes, dtype=torch.float32) # log probability for the random initial solution\n",
    "        trajectory_advantages[0, :] = torch.zeros(trajectories_per_epoch)\n",
    "        ls_trajectory_solutions[0, :, :] = torch.randint(2, (trajectories_per_epoch, self.num_nodes), dtype=torch.float32).to(self.device)\n",
    "\n",
    "\n",
    "        return trajectory_solutions, trajectory_scores, trajectory_log_probs, trajectory_advantages, ls_trajectory_solutions\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0d184",
   "metadata": {},
   "source": [
    "# Policy Gradient NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f83d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected network with one hidden layer, and a sigmoid function to output probability\n",
    "class FCPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FCPolicy, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        #layers.append(nn.Softmax(dim=1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, mode=\"sample\"):\n",
    "        return self.model(x)\n",
    "    \n",
    "# Solution-level RNN model\n",
    "class RNNPolicy_Solution(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(RNNPolicy_Solution, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, mode='sample'):\n",
    "        # x has shape: (trajectory_length, batch_size, input_dim)\n",
    "        rnn_output, _ = self.rnn(x)\n",
    "        # rnn_output has shape: (trajectory_length, batch_size, hidden_dim)\n",
    "\n",
    "        # For sampling the next solution, only take the last hidden state\n",
    "        if mode=='sample':\n",
    "            rnn_output = rnn_output[-1, :, :]\n",
    "\n",
    "        # Pass the hidden state through fc+sigmoid to get probability\n",
    "        linear_output = self.fc(rnn_output)\n",
    "        prob = self.sigmoid(linear_output)\n",
    "\n",
    "        # prob has shape: (batch_size, input_dim)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259b868",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6564ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer implementation\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_size, trajectory_length, num_nodes, device=torch.device(\"cpu\")):\n",
    "        self.solutions = torch.empty((trajectory_length, buffer_size, num_nodes), dtype=torch.float32)\n",
    "        self.scores = torch.empty((trajectory_length, buffer_size), dtype=torch.float32)\n",
    "        self.log_probs = torch.empty((trajectory_length, buffer_size), dtype=torch.float32)\n",
    "        self.advantages = torch.empty((trajectory_length, buffer_size), dtype=torch.float32)\n",
    "\n",
    "        self.p = 0\n",
    "        self.buffer_size = buffer_size\n",
    "        self.device = device\n",
    "        self.num_nodes = num_nodes\n",
    "\n",
    "    # Add new batch of trajectories.\n",
    "    # Replace the trajectories with lowest score sums if the buffer is full\n",
    "    def update(self, solutions, scores, log_probs, advantages):\n",
    "        # solutions with size (trajectory_length, trajectories_per_epoch, num_nodes)\n",
    "        # scores and log_probs with size (trajectory_length, trajectories_per_epoch)\n",
    "        add_size = scores.shape[1]\n",
    "\n",
    "        # Add trajectories\n",
    "        if self.p+add_size < self.buffer_size:\n",
    "            self.solutions[:, self.p:self.p+add_size, :] = solutions.to(self.device)\n",
    "            self.scores[:, self.p:self.p+add_size] = scores.to(self.device)\n",
    "            self.log_probs[:, self.p:self.p+add_size] = log_probs.to(self.device)\n",
    "            self.advantages[:, self.p:self.p+add_size] = advantages.to(self.device)\n",
    "            self.p += add_size\n",
    "\n",
    "        # when the buffer is full, replace the batch of trajectories with lowest scores\n",
    "        else:\n",
    "            # Calculate score-sum for each trajectory\n",
    "            trajectory_scores = self.scores[:, :self.p].sum(dim=0)\n",
    "            _, ids = torch.topk(trajectory_scores, k=add_size, largest=False)\n",
    "\n",
    "            # Replace the lowest trajectories with the new batch\n",
    "            self.solutions[:,ids, :] = solutions.to(self.device)\n",
    "            self.scores[:,ids] = scores.to(self.device)\n",
    "            self.log_probs[:,ids] = log_probs.to(self.device)\n",
    "            self.advantages[:,ids] = advantages.to(self.device)\n",
    "\n",
    "    # Sample the top batch_size//4 trajectories and fill the rest of the batch with random ones\n",
    "    def sample(self, batch_size, device):\n",
    "        top_k = batch_size // 4\n",
    "        trajectory_scores = self.scores[:, :self.p].sum(dim=0)\n",
    "        # Get top_k trajectory ids\n",
    "        _, top_ids = torch.topk(trajectory_scores, k=top_k, largest=True)\n",
    "\n",
    "        # Exclude topk-ids to avoid repeated sampling\n",
    "        all_ids = torch.arange(self.p)\n",
    "        remaining_ids = all_ids[~torch.isin(all_ids, top_ids)]\n",
    "        random_k = batch_size - top_k\n",
    "\n",
    "        # Random sample without replacement from remaining ids\n",
    "        random_ids = remaining_ids[\n",
    "            torch.randperm(len(remaining_ids))[:random_k]\n",
    "        ]\n",
    "        # Combine top and random indices\n",
    "        ids = torch.cat([top_ids, random_ids], dim=0)\n",
    "\n",
    "        return (self.solutions[:, ids, :].to(device),\n",
    "                self.scores[:, ids].to(device),\n",
    "                self.log_probs[:, ids].to(device),\n",
    "                self.advantages[:, ids].to(device))\n",
    "\n",
    "    # returns the best batch of solutions in the buffer\n",
    "    def get_top_solutions(self, batch_size, device):\n",
    "        trajectory_length = self.scores.shape[0]\n",
    "        flat_scores = self.scores.flatten() # shape (trajectory_length*buffer_size)\n",
    "        flat_solutions = self.solutions.view((trajectory_length*self.buffer_size, self.num_nodes)) # shape (trajectory_length*buffer_size, num_nodes)\n",
    "        _, ids = torch.topk(flat_scores, k=batch_size, largest=True)\n",
    "        return flat_solutions[ids, :].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30b2e6",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ecac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl_stages = [(ls_num_flips, ls_max_steps, ls_num_neighbors, lambda_l)]\n",
    "def train(maxcut_simulator, actor_model, actor_optimizer, num_epochs = 300, update_steps = 6,\n",
    "          buffer_size = 32, batch_size = 32, trajectory_length = 32, trajectories_per_epoch = 32, \n",
    "          lambda_l = 0.3, model_choice='FC', gamma = 0.99, ls_num_flips=6, ls_num_neighbors = 3,\n",
    "          ls_max_steps = 3, device = torch.device(\"cpu\")):\n",
    "\n",
    "    # Initialize experiment\n",
    "\n",
    "    num_nodes = maxcut_simulator.num_nodes\n",
    "    buffer = Buffer(buffer_size, trajectory_length, num_nodes) # initialize buffer\n",
    "\n",
    "    best_score = 0 # Used to track best score\n",
    "    best_solution = [] # Used to track best solution\n",
    "    best_epoch = 0 # Used to track best epoch\n",
    "    scores = [] # Used to track scores at each epoch\n",
    "    solutions = [] # Used to track the solutions at each epoch\n",
    "    actor_stats_losses = [] # store losses\n",
    "    critic_stats_losses = []\n",
    "\n",
    "    mse_loss = torch.nn.MSELoss() # MSE for critic loss\n",
    "\n",
    "    # Start training\n",
    "    for k in tqdm(range(num_epochs)):\n",
    "        epoch_scores = [] # Store (max) cut values for an epoch\n",
    "        epoch_solutions = [] # Store best solution for an epoch\n",
    "\n",
    "        # initialize tensors to store batch of trajectories for this epoch\n",
    "        (\n",
    "            trajectory_solutions,\n",
    "            trajectory_scores,\n",
    "            trajectory_log_probs,\n",
    "            trajectory_advantages,\n",
    "            ls_trajectory_solutions\n",
    "        ) = maxcut_simulator.initialize_trajectories(\n",
    "            trajectory_length,\n",
    "            trajectories_per_epoch\n",
    "        )\n",
    "\n",
    "        # Sample a batch of trajectories\n",
    "        for t in range(1, trajectory_length):\n",
    "            with torch.no_grad():\n",
    "                if model_choice == \"RNN\":\n",
    "                    outputs = actor_model(trajectory_solutions[:t])\n",
    "                else:\n",
    "                    outputs = actor_model(trajectory_solutions[t-1])\n",
    "\n",
    "                # Sample new solution from outputs\n",
    "                m = torch.distributions.Bernoulli(probs=outputs)\n",
    "                new_solutions = m.sample()\n",
    "\n",
    "                # calculate the log probability of this solution\n",
    "                log_probs = m.log_prob(new_solutions).sum(dim=1)\n",
    "\n",
    "                # calculate cut value of the new solution\n",
    "                new_scores = maxcut_simulator.get_cut_values(new_solutions)\n",
    "\n",
    "                #local search\n",
    "                if ls_num_flips*ls_max_steps > 0:\n",
    "                    new_solutions, new_scores = maxcut_simulator.batched_local_search(\n",
    "                                                            new_solutions,\n",
    "                                                            new_scores,\n",
    "                                                            ls_max_steps,\n",
    "                                                            ls_num_flips,\n",
    "                                                            ls_num_neighbors)\n",
    "\n",
    "                bs = max(new_scores)\n",
    "                if bs > best_score:\n",
    "                    best_score = bs.tolist()\n",
    "                    best_solution = new_solutions[torch.argmax(new_scores)].tolist()\n",
    "                    best_epoch = k\n",
    "\n",
    "                trajectory_solutions[t, :, :] = new_solutions\n",
    "                trajectory_scores[t, :] = new_scores\n",
    "                trajectory_log_probs[t, :] = log_probs\n",
    "                trajectory_advantages[t, :] = new_scores - trajectory_scores[t-1]\n",
    "\n",
    "                epoch_scores += (new_scores.tolist())\n",
    "                epoch_solutions += (new_solutions.tolist())\n",
    "\n",
    "        # Store scores and solutions of the epoch\n",
    "        scores.append(trajectory_scores.tolist())\n",
    "        solutions.append(trajectory_solutions.tolist())\n",
    "\n",
    "        # Store the new trajectories score and log_prob in the buffer\n",
    "        buffer.update(trajectory_solutions,\n",
    "                        trajectory_scores,\n",
    "                        trajectory_log_probs,\n",
    "                        trajectory_advantages)\n",
    "\n",
    "        # Sample from buffer after all trajectories for this epoch are added\n",
    "        if buffer.p >= batch_size*update_steps//2:\n",
    "            # update a fixed amount of steps\n",
    "            actor_losses = []\n",
    "            for j in range(update_steps):\n",
    "                layer_i = j % 3\n",
    "                # Sample a batch from buffer\n",
    "                batch_trajectories, batch_scores, batch_log_probs, batch_advantages = buffer.sample(batch_size, device)\n",
    "\n",
    "                # Compute log prob of trajectory based on current policy\n",
    "                outputs = actor_model(batch_trajectories, mode='update')                \n",
    "\n",
    "                m = torch.distributions.Bernoulli(probs=outputs)\n",
    "                solution_log_probs = m.log_prob(batch_trajectories).sum(dim=-1) # get solution log probs\n",
    "                trajectory_log_probs = solution_log_probs.sum(0) # get trajectory log probs\n",
    "                # trajectory_scores = batch_scores.sum(0) # get trajectory scores\n",
    "\n",
    "                trajectory_log_prob = trajectory_log_probs.mean() # get batch mean trajectory log prob\n",
    "                # trajectory_score = trajectory_scores.mean() # get batch mean trajectory score\n",
    "\n",
    "                trajectory_scores = batch_scores.sum(0)\n",
    "                trajectory_score = trajectory_scores.mean()\n",
    "\n",
    "                trajectory_advantages = batch_advantages.sum(0) # get batch mean trajectory advantages\n",
    "                trajectory_advantage = trajectory_advantages.mean()\n",
    "\n",
    "                # Objective to maximize trajectory probability and score, with an entropy regularization controlling exploration\n",
    "                actor_loss = -(trajectory_log_prob * trajectory_advantage) - lambda_l * trajectory_log_prob\n",
    "\n",
    "                # Update actor model\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                # torch.nn.utils.clip_grad_norm_(actor_model.parameters(), max_norm=1.0)\n",
    "                actor_optimizer.step()\n",
    "                actor_losses.append(actor_loss.item())\n",
    "\n",
    "            actor_stats_losses.append(np.mean(actor_losses))\n",
    "\n",
    "    return best_score, best_solution, best_epoch, scores, solutions, epoch_scores, actor_stats_losses, critic_stats_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2211cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Hyperparameters\n",
    "num_epochs = 600\n",
    "update_steps = 4\n",
    "trajectory_length = 32\n",
    "trajectories_per_epoch = 32\n",
    "buffer_size = 1024\n",
    "batch_size = 32\n",
    "lambda_l = 0.3\n",
    "model_choice = 'FC' # 'FC' or 'RNN'\n",
    "\n",
    "# Variables used for graph fetching and saving\n",
    "input_graph = \"../data/syn_BA/BA_100_ID0.txt\"\n",
    "output_location = \"../results/BA_100_ID0\"\n",
    "params = ''\n",
    "\n",
    "\n",
    "# Load data, initialize the simulator and hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "graph_instance = input_graph.split('/')[-1].split('.txt')[0]\n",
    "maxcut_simulator = MaxcutSimulator(input_graph, device=device)\n",
    "num_nodes = maxcut_simulator.num_nodes\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "input_dim = num_nodes\n",
    "output_dim = num_nodes\n",
    "hidden_dim = trajectories_per_epoch\n",
    "\n",
    "if model_choice == 'FC':\n",
    "     actor_model = FCPolicy(input_dim, hidden_dim, output_dim).to(device)\n",
    "if model_choice == 'RNN':\n",
    "    actor_model = RNNPolicy_Solution(input_dim, hidden_dim).to(device)\n",
    "actor_optimizer = torch.optim.Adam(actor_model.parameters())\n",
    "\n",
    "\n",
    "(\n",
    "    best_score,\n",
    "    best_solution,\n",
    "    best_epoch,\n",
    "    scores,\n",
    "    solutions,\n",
    "    epoch_scores,\n",
    "    actor_stats_losses,\n",
    "    critic_stats_losses,\n",
    ")= train(maxcut_simulator, actor_model, actor_optimizer, num_epochs=num_epochs, update_steps=update_steps, \n",
    "         buffer_size=buffer_size, batch_size=batch_size, trajectory_length=trajectory_length, \n",
    "         trajectories_per_epoch=trajectories_per_epoch, lambda_l= lambda_l, model_choice=model_choice, device = device)\n",
    "\n",
    "print(f\"\\nlast epoch best score: {max(epoch_scores)}, overall best score: {best_score} at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d70f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.tensor(scores)\n",
    "mean_scores = scores[:, -1].mean(-1).tolist() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebc3ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cut-values\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "#smooth_scores = savgol_filter(scores, window_length=1, polyorder=0)\n",
    "plt.plot(mean_scores)\n",
    "\n",
    "graph_dir = output_location + f\"/{model_choice}_images\"\n",
    "if not os.path.exists(graph_dir):\n",
    "    os.makedirs(graph_dir)\n",
    "\n",
    "#plt.plot(scores, alpha=0.3, label='Original Scores')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Score')\n",
    "plt.title(f'{model_choice}_training_results')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e610faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "import numpy as np\n",
    "plt.plot(actor_stats_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average losses')\n",
    "plt.title(f'{model_choice}: Actor Losses vs Epoch')\n",
    "plt.savefig(f'{graph_dir}/Actor_Losses_Epoch.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
